// --- TRANSLATIONS ---
window.translations = {
    en: {
        languageName: "English",
        crop_analyzer_title: "Crop Disease AI Analyzer",
        crop_analyzer_subtitle: "Use your camera to identify crop diseases and get instant solutions.",
        start_camera: "Start Camera",
        switch_camera: "Switch Camera",
        restart_camera: "Retake Photo",
        capture_photo: "Capture Photo",
        chat_with_agri_ai: "Chat with Agri-AI",
        placeholder_initial: "Click \"Start Camera\" to begin.",
        placeholder_after_capture: "Ask the AI about the captured image...",
        ask_about_image: "Ask about the image...",
        send: "Send",
        read_response: "Read Last Response",
        analyzing: "Analyzing...",
        listening: "Listening...",
        input_set_to: "Input set to: ",
        mic_error: "Mic Error: ",
        speech_not_supported: "Speech recognition not supported by your browser.",
        api_key_error: "API key is not set. Please add your key in logic.js",
        tts_generating: "Generating audio...",
        tts_error: "Could not generate audio.",
        tts_fast: "Fast",
        tts_high_quality: "High Quality",
        camera_placeholder: "Camera feed will appear here",
        camera_requesting: "Requesting camera access...",
        camera_error_generic: "Could not access the camera. Please ensure permissions are granted and try refreshing.",
        camera_error_denied: "Camera access denied. Please grant permission in your browser settings.",
        camera_error_not_found: "No camera was found on your device."
    },
    hi: {
        languageName: "Hindi",
        crop_analyzer_title: "फसल रोग एआई विश्लेषक",
        crop_analyzer_subtitle: "फसल रोगों की पहचान करने और तुरंत समाधान पाने के लिए अपने कैमरे का उपयोग करें।",
        start_camera: "कैमरा शुरू करें",
        switch_camera: "कैमरा बदलें",
        restart_camera: "फोटो फिर से लें",
        capture_photo: "फोटो खींचें",
        chat_with_agri_ai: "कृषि-एआई से चैट करें",
        placeholder_initial: "\"कैमरा शुरू करें\" पर क्लिक करें।",
        placeholder_after_capture: "एआई से खींची गई छवि के बारे में पूछें...",
        ask_about_image: "छवि के बारे में पूछें...",
        send: "भेजें",
        read_response: "अंतिम प्रतिक्रिया पढ़ें",
        analyzing: "विश्लेषण हो रहा है...",
        listening: "सुन रहा हूँ...",
        input_set_to: "इनपुट इस पर सेट है: ",
        mic_error: "माइक त्रुटि: ",
        speech_not_supported: "आपका ब्राउज़र वाक् पहचान का समर्थन नहीं करता है।",
        api_key_error: "API कुंजी सेट नहीं है। कृपया logic.js में अपनी कुंजी जोड़ें",
        tts_generating: "ऑडियो बना रहा है...",
        tts_error: "ऑडियो उत्पन्न नहीं हो सका।",
        tts_fast: "तेज़",
        tts_high_quality: "उच्च गुणवत्ता",
        camera_placeholder: "कैमरा फ़ीड यहाँ दिखाई देगी",
        camera_requesting: "कैमरे तक पहुंच का अनुरोध किया जा रहा है...",
        camera_error_generic: "कैमरे तक नहीं पहुंच सका। कृपया सुनिश्चित करें कि अनुमति दी गई है और पुनः प्रयास करें।",
        camera_error_denied: "कैमरे तक पहुंच से इनकार कर दिया गया। कृपया अपनी ब्राउज़र सेटिंग्स में अनुमति दें।",
        camera_error_not_found: "आपके डिवाइस पर कोई कैमरा नहीं मिला।"
    },
    ml: {
        languageName: "Malayalam",
        crop_analyzer_title: "വിള രോഗ എഐ അനലൈസർ",
        crop_analyzer_subtitle: "വിള രോഗങ്ങൾ തിരിച്ചറിയാനും തൽക്ഷണ പരിഹാരങ്ങൾ നേടാനും നിങ്ങളുടെ ക്യാമറ ഉപയോഗിക്കുക.",
        start_camera: "ക്യാമറ ആരംഭിക്കുക",
        switch_camera: "ക്യാമറ മാറ്റുക",
        restart_camera: "ചിത്രം വീണ്ടും എടുക്കുക",
        capture_photo: "ചിത്രം പകർത്തുക",
        chat_with_agri_ai: "അഗ്രി-എഐയുമായി ചാറ്റ് ചെയ്യുക",
        placeholder_initial: "ആരംഭിക്കാൻ \"ക്യാമറ ആരംഭിക്കുക\" ക്ലിക്കുചെയ്യുക.",
        placeholder_after_capture: "പകർത്തിയ ചിത്രത്തെക്കുറിച്ച് AI-യോട് ചോദിക്കുക...",
        ask_about_image: "ചിത്രത്തെക്കുറിച്ച് ചോദിക്കുക...",
        send: "അയയ്ക്കുക",
        read_response: "അവസാന പ്രതികരണം വായിക്കുക",
        analyzing: "വിശകലനം ചെയ്യുന്നു...",
        listening: "കേൾക്കുന്നു...",
        input_set_to: "ഇൻപുട്ട് ഇതിലേക്ക് സജ്ജമാക്കി: ",
        mic_error: "മൈക്ക് പിശക്: ",
        speech_not_supported: "നിങ്ങളുടെ ബ്രൗസർ സംഭാഷണം തിരിച്ചറിയുന്നതിനെ പിന്തുണയ്ക്കുന്നില്ല.",
        api_key_error: "API കീ സജ്ജീകരിച്ചിട്ടില്ല. ദയവായി നിങ്ങളുടെ കീ logic.js-ൽ ചേർക്കുക",
        tts_generating: "ഓഡിയോ സൃഷ്ടിക്കുന്നു...",
        tts_error: "ഓഡിയോ സൃഷ്ടിക്കാൻ കഴിഞ്ഞില്ല.",
        tts_fast: "വേഗതയേറിയത്",
        tts_high_quality: "ഉയർന്ന നിലവാരം",
        camera_placeholder: "ക്യാമറ ഫീഡ് ഇവിടെ ദൃശ്യമാകും",
        camera_requesting: "ക്യാമറ ആക്‌സസ് അഭ്യർത്ഥിക്കുന്നു...",
        camera_error_generic: "ക്യാമറ ആക്‌സസ് ചെയ്യാൻ കഴിഞ്ഞില്ല. അനുമതികൾ നൽകിയിട്ടുണ്ടെന്ന് ഉറപ്പുവരുത്തി വീണ്ടും ശ്രമിക്കുക.",
        camera_error_denied: "ക്യാമറ ആക്‌സസ് നിരസിച്ചു. ദയവായി നിങ്ങളുടെ ബ്രൗസർ ക്രമീകരണങ്ങളിൽ അനുമതി നൽകുക.",
        camera_error_not_found: "നിങ്ങളുടെ ഉപകരണത്തിൽ ക്യാമറയൊന്നും കണ്ടെത്തിയില്ല."
    }
};

// --- CONFIG ---
const API_KEY = "AIzaSyDXQMEjuQFAsFKEnTmo7SjO5csWkqbwPsM";
// --- END CONFIG ---

document.addEventListener('DOMContentLoaded', () => {

    // DOM Elements
    const video = document.getElementById('videoElement');
    const canvas = document.getElementById('canvasElement');
    const capturedImage = document.getElementById('capturedImage');
    const startCamBtn = document.getElementById('startCamBtn');
    const captureBtn = document.getElementById('captureBtn');
    const micBtn = document.getElementById('micBtn');
    const micStatus = document.getElementById('micStatus');
    const speakBtn = document.getElementById('speakBtn');
    const ttsStatus = document.getElementById('ttsStatus');
    const placeholder = document.getElementById('placeholder');
    const chatWindow = document.getElementById('chat-window');
    const chatInput = document.getElementById('chatInput');
    const sendBtn = document.getElementById('sendBtn');
    const languageSelector = document.getElementById('languageSelector');
    const ttsQualityToggle = document.getElementById('ttsQualityToggle');
    const camStatus = document.getElementById('camStatus');
    const videoPlaceholder = document.getElementById('videoPlaceholder');

    // State
    let stream = null;
    let capturedImageData = null;
    let chatHistory = [];
    let lastAIResponse = "";
    let isAwaitingAIResponse = false;
    let isGeneratingSpeech = false;
    let selectedLanguage = 'en';
    let audioContext = null;
    let ttsQuality = 'fast';

    // --- SPEECH RECOGNITION SETUP ---
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    let recognition = null;
    if (SpeechRecognition) {
        recognition = new SpeechRecognition();
        recognition.continuous = false;
        recognition.lang = 'en-US';
        recognition.interimResults = false;
        recognition.maxAlternatives = 1;

        recognition.onstart = () => {
            micStatus.textContent = translations[selectedLanguage].listening;
            micBtn.classList.add('pulsing-mic', 'bg-red-500', 'text-white');
        };
        recognition.onresult = (event) => {
            const spokenText = event.results[0][0].transcript;
            chatInput.value = spokenText;
            micStatus.textContent = `${translations[selectedLanguage].input_set_to}"${spokenText}"`;
        };
        recognition.onerror = (event) => {
            micStatus.textContent = `${translations[selectedLanguage].mic_error} ${event.error}`;
            console.error("Speech Recognition Error - doclogic.js:152", event);
        };
        recognition.onend = () => {
            micBtn.classList.remove('pulsing-mic', 'bg-red-500', 'text-white');
            setTimeout(() => { if (!micStatus.textContent.includes('Error')) micStatus.textContent = ''; }, 3000);
        };
    } else {
        micBtn.disabled = true;
    }
    
    // --- EVENT LISTENERS ---
    startCamBtn.addEventListener('click', startCamera);
    captureBtn.addEventListener('click', capturePhoto);
    sendBtn.addEventListener('click', handleSendMessage);
    chatInput.addEventListener('keydown', (e) => {
        if (e.key === 'Enter' && !e.shiftKey) {
            e.preventDefault();
            handleSendMessage();
        }
    });
    micBtn.addEventListener('click', () => {
        if (recognition && !micBtn.classList.contains('pulsing-mic')) {
            try {
                recognition.lang = selectedLanguage === 'en' ? 'en-US' : selectedLanguage === 'hi' ? 'hi-IN' : 'ml-IN';
                recognition.start();
            } catch (e) {
                console.error("Error starting recognition: - doclogic.js:178", e);
                micStatus.textContent = `${translations[selectedLanguage].mic_error} could not start.`;
            }
        }
    });
    speakBtn.addEventListener('click', speakResult);
    languageSelector.addEventListener('change', handleLanguageChange);
    ttsQualityToggle.addEventListener('change', (e) => {
        ttsQuality = e.target.checked ? 'high' : 'fast';
    });

    // --- CORE FUNCTIONS ---
    function translateUI() {
        document.querySelectorAll('[data-translate-key]').forEach(el => {
            const key = el.dataset.translateKey;
            const translation = translations[selectedLanguage][key];
            if (el.tagName === 'INPUT') {
                el.placeholder = translation;
            } else {
                el.textContent = translation;
            }
        });
        if (!SpeechRecognition) {
            micStatus.textContent = translations[selectedLanguage].speech_not_supported;
        }
    }

    function handleLanguageChange(event) {
        selectedLanguage = event.target.value;
        ttsStatus.textContent = '';
        translateUI();
    }

    async function startCamera() {
        if (stream) stream.getTracks().forEach(track => track.stop());

        camStatus.textContent = translations[selectedLanguage].camera_requesting;
        videoPlaceholder.style.display = 'none';
        
        try {
            const constraints = { video: { facingMode: 'environment' } };
            stream = await navigator.mediaDevices.getUserMedia(constraints).catch(async (e) => {
                console.warn("Environment camera failed, trying user camera: - doclogic.js:220", e);
                return await navigator.mediaDevices.getUserMedia({ video: true });
            });

            video.srcObject = stream;
            video.style.display = 'block';
            capturedImage.style.display = 'none';
            captureBtn.disabled = false;
            startCamBtn.dataset.translateKey = "switch_camera";
            camStatus.textContent = '';
        } catch (err) {
            console.error("Error accessing camera: - doclogic.js:231", err);
            let errorKey;
            if (err.name === "NotAllowedError" || err.name === "PermissionDeniedError") {
                errorKey = "camera_error_denied";
            } else if (err.name === "NotFoundError" || err.name === "DevicesNotFoundError") {
                errorKey = "camera_error_not_found";
            } else {
                errorKey = "camera_error_generic";
            }
            camStatus.textContent = translations[selectedLanguage][errorKey];
            videoPlaceholder.style.display = 'flex';
        } finally {
            translateUI();
        }
    }

    function capturePhoto() {
        if (!stream) return;

        try {
            const track = stream.getVideoTracks()[0];
            const settings = track.getSettings();
            canvas.width = settings.width;
            canvas.height = settings.height;
            const context = canvas.getContext('2d');
            context.drawImage(video, 0, 0, canvas.width, canvas.height);
            const fullDataURL = canvas.toDataURL('image/jpeg', 0.9);
            capturedImageData = fullDataURL.split(',')[1];

            if (!capturedImageData) throw new Error("Failed to capture image data");

            capturedImage.src = fullDataURL;
            video.style.display = 'none';
            capturedImage.style.display = 'block';
            chatInput.disabled = false;
            sendBtn.disabled = false;
            micBtn.disabled = !recognition;
            placeholder.querySelector('p').dataset.translateKey = "placeholder_after_capture";
            
            stream.getTracks().forEach(track => track.stop());
            startCamBtn.dataset.translateKey = "restart_camera";
            chatHistory = [];
            lastAIResponse = "";
            speakBtn.disabled = true;
            ttsStatus.textContent = '';
        } catch (error) {
            console.error("Error capturing photo: - doclogic.js:277", error);
            appendMessage("Failed to capture photo. Please try again.", 'ai', true);
        } finally {
            translateUI();
        }
    }

    function handleSendMessage() {
        const userText = chatInput.value.trim();
        if (!userText || isAwaitingAIResponse) return;
        
        appendMessage(userText, 'user');
        
        const userMessage = { role: 'user', parts: [{ text: userText }] };
        if (chatHistory.length === 0 && capturedImageData) {
            userMessage.parts.unshift({ inlineData: { mimeType: "image/jpeg", data: capturedImageData } });
        }
        
        chatHistory.push(userMessage);
        chatInput.value = '';
        getAIResponse();
    }

    async function getAIResponse() {
        if (!API_KEY) {
            appendMessage(translations[selectedLanguage].api_key_error, 'ai', true);
            return;
        }

        isAwaitingAIResponse = true;
        setChatControls(false);
        appendMessage('', 'loader');
        
        const API_URL = `/api/gemini`;
        const systemPrompt = `You are a specialized agricultural AI assistant. Your task is to identify crop diseases from the user's image and answer questions. Provide concise, actionable solutions. Format your response clearly. When identifying a disease, include 'Disease Name', 'Confidence Level', 'Description', and a 'Recommended Solution' with organic and chemical options if applicable. For follow-up questions, be helpful and conversational. CRUCIAL: Your entire response must be in ${translations[selectedLanguage].languageName} (language code: ${selectedLanguage}). Do not use any other language.`;
        const payload = { model: "gemini-2.5-flash-preview-05-20", contents: chatHistory, systemInstruction: { parts: [{ text: systemPrompt }] } };

        try {
            const response = await fetchWithBackoff(API_URL, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
            });

            if (!response.ok) {
                const errorBody = await response.json().catch(() => ({}));
                const detail = errorBody.error?.message || response.statusText;
                throw new Error(`API Error ${response.status}: ${detail}. Check if the API key is valid and has access.`);
            }

            const result = await response.json();
            const candidate = result.candidates?.[0];
            
            if (!candidate || !candidate.content?.parts) {
                const finishReason = candidate?.finishReason || "UNKNOWN";
                const safetyMessage = `Response blocked (${finishReason}). Try a different image or question.`;
                throw new Error(safetyMessage);
            }

            const aiText = candidate.content.parts[0].text;
            document.querySelector('.loader-container')?.remove();
            appendMessage(aiText, 'ai');
            chatHistory.push({ role: 'model', parts: [{ text: aiText }] });
            lastAIResponse = aiText;
            speakBtn.disabled = false;

        } catch (error) {
            document.querySelector('.loader-container')?.remove();
            console.error("Error during AI diagnosis: - doclogic.js:345", error);
            appendMessage(`An error occurred: ${error.message}`, 'ai', true);
        } finally {
            isAwaitingAIResponse = false;
            setChatControls(true);
        }
    }

    function setChatControls(enabled) {
        chatInput.disabled = !enabled;
        sendBtn.disabled = !enabled;
        if (enabled) chatInput.focus();
    }

    // --- HELPER & UI FUNCTIONS ---
    function appendMessage(text, sender, isError = false) {
        if (placeholder) placeholder.style.display = 'none';
        const messageWrapper = document.createElement('div');
        messageWrapper.className = `w-full flex ${sender === 'user' ? 'justify-end' : 'justify-start'}`;

        if (sender === 'loader') {
            messageWrapper.innerHTML = `<div class="loader-container flex items-center gap-2 text-sm text-gray-500 bg-gray-200 p-2 rounded-lg"><div class="loader" style="width:20px; height:20px; border-width: 2px;"></div><span>${translations[selectedLanguage].analyzing}</span></div>`;
        } else {
            const messageBubble = document.createElement('div');
            messageBubble.className = `message-bubble p-3 rounded-lg max-w-[80%] ${sender === 'user' ? 'bg-blue-600 text-white' : (isError ? 'bg-red-100 text-red-800' : 'bg-gray-200 text-gray-800')}`;
            messageBubble.innerHTML = text.replace(/(\*\*|__)(.*?)\1/g, '<strong>$2</strong>').replace(/\*(.*?)\*/g, '<em>$1</em>').replace(/`([^`]+)`/g, '<code>$1</code>').replace(/\n/g, '<br>');
            messageWrapper.appendChild(messageBubble);
        }
        chatWindow.appendChild(messageWrapper);
        chatWindow.scrollTop = chatWindow.scrollHeight;
    }

    async function fetchWithBackoff(url, options, retries = 3) {
        let delay = 1000;
        for (let i = 0; i < retries; i++) {
            try {
                const response = await fetch(url, options);
                if (response.status !== 429) return response;
                console.warn(`Rate limited. Retrying in ${delay / 1000}s... - doclogic.js:383`);
            } catch (error) {
                if (i === retries - 1) throw error;
            }
            await new Promise(resolve => setTimeout(resolve, delay));
            delay *= 2;
        }
        throw new Error("Max retries reached.");
    }
    
    // --- TTS FUNCTIONS ---
    async function speakResult() {
        if (isGeneratingSpeech || !lastAIResponse) return;
        isGeneratingSpeech = true;
        speakBtn.disabled = true;
        ttsStatus.textContent = translations[selectedLanguage].tts_generating;

        try {
            if (ttsQuality === 'high') {
                await speakWithGemini();
            } else {
                await speakWithBrowser();
            }
        } catch (error) {
            console.error('TTS Error: - doclogic.js:407', error);
            ttsStatus.textContent = translations[selectedLanguage].tts_error;
        } finally {
            isGeneratingSpeech = false;
            speakBtn.disabled = false;
            setTimeout(() => { if (ttsStatus.textContent !== '') ttsStatus.textContent = ''; }, 4000);
        }
    }
    
    async function speakWithGemini() {
        if (!API_KEY) throw new Error("API key not provided for TTS.");
        
        const TTS_API_URL = `/api/gemini`;
        if (!audioContext) audioContext = new (window.AudioContext || window.webkitAudioContext)();

        const payload = {
            model: "gemini-2.5-flash-preview-tts",
            contents: [{ parts: [{ text: `Say this in a clear, helpful tone: ${lastAIResponse}` }] }],
            generationConfig: { responseModalities: ["AUDIO"], speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: "Charon" } } } }
        };
        
        const response = await fetchWithBackoff(TTS_API_URL, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) });
        if (!response.ok) throw new Error(`TTS API Error: ${response.status}`);
        
        const result = await response.json();
        const audioData = result?.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
        const mimeType = result?.candidates?.[0]?.content?.parts?.[0]?.inlineData?.mimeType;

        if (!audioData || !mimeType?.startsWith("audio/")) throw new Error("Invalid audio data from API.");

        const sampleRate = parseInt(mimeType.match(/rate=(\d+)/)?.[1] || "24000", 10);
        const pcmData = base64ToArrayBuffer(audioData);
        const wavBlob = pcmToWav(pcmData, sampleRate);
        const audioBuffer = await audioContext.decodeAudioData(await wavBlob.arrayBuffer());
        
        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioContext.destination);
        source.start(0);
        
        return new Promise(resolve => { source.onended = resolve; });
    }

    function speakWithBrowser() {
        return new Promise((resolve, reject) => {
            if (!('speechSynthesis' in window)) return reject(new Error("Speech synthesis not supported."));

            window.speechSynthesis.cancel();
            const utterance = new SpeechSynthesisUtterance(lastAIResponse);
            utterance.lang = selectedLanguage;
            
            const voices = window.speechSynthesis.getVoices();
            utterance.voice = voices.find(voice => voice.lang.startsWith(selectedLanguage)) || voices.find(voice => voice.lang.startsWith('en'));

            utterance.onend = resolve;
            utterance.onerror = (event) => reject(event.error);
            
            window.speechSynthesis.speak(utterance);
        });
    }

    function base64ToArrayBuffer(base64) {
        const binaryString = window.atob(base64);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
            bytes[i] = binaryString.charCodeAt(i);
        }
        return bytes.buffer;
    }

    function pcmToWav(pcmData, sampleRate) {
        const buffer = new ArrayBuffer(44 + pcmData.byteLength);
        const view = new DataView(buffer);
        // RIFF header
        view.setUint32(0, 0x52494646, false); // "RIFF"
        view.setUint32(4, 36 + pcmData.byteLength, true);
        view.setUint32(8, 0x57415645, false); // "WAVE"
        // "fmt " sub-chunk
        view.setUint32(12, 0x666d7420, false); // "fmt "
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, 1, true); // numChannels
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * 2, true); // byteRate
        view.setUint16(32, 2, true); // blockAlign
        view.setUint16(34, 16, true); // bitsPerSample
        // "data" sub-chunk
        view.setUint32(36, 0x64617461, false); // "data"
        view.setUint32(40, pcmData.byteLength, true);
        // Write PCM data
        new Uint8Array(buffer, 44).set(new Uint8Array(pcmData));
        return new Blob([view], { type: 'audio/wav' });
    }

    // --- INITIALIZATION ---
    if ('speechSynthesis' in window && window.speechSynthesis.onvoiceschanged !== undefined) {
         window.speechSynthesis.onvoiceschanged = () => window.speechSynthesis.getVoices();
    }
    translateUI();
});

